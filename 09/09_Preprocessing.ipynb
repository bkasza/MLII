{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "g35j8ZJXdIAv"
      },
      "source": [
        "## Inicjalizacja środowiska programistycznego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Jl7ggmzddIAw"
      },
      "outputs": [],
      "source": [
        "#Color printing\n",
        "from termcolor import colored\n",
        "\n",
        "#General data operations library\n",
        "import math, string, glob\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "#The tensorflow library\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
        "import tensorflow  as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "#Plotting libraries\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Increase plots font size\n",
        "params = {'legend.fontsize': 'xx-large',\n",
        "          'figure.figsize': (10, 7),\n",
        "         'axes.labelsize': 'xx-large',\n",
        "         'axes.titlesize':'xx-large',\n",
        "         'xtick.labelsize':'xx-large',\n",
        "         'ytick.labelsize':'xx-large'}\n",
        "plt.rcParams.update(params)\n",
        "\n",
        "#append path with python modules\n",
        "import importlib\n",
        "import sys\n",
        "# sys.path.append(\"../modules\")\n",
        "\n",
        "#Private functions\n",
        "# import plotting_functions as plf\n",
        "# importlib.reload(plf);\n",
        "\n",
        "# import text_functions as txt_fcn\n",
        "# importlib.reload(txt_fcn);\n",
        "#Hide GPU\n",
        "#tf.config.set_visible_devices([], 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ivRqkV_vdIAx"
      },
      "source": [
        "<br/><br/>\n",
        "<br/><br/>\n",
        "\n",
        "<h1 align=\"center\">\n",
        " Uczenie maszynowe II\n",
        "</h1>\n",
        "\n",
        "<br/><br/>\n",
        "<br/><br/>\n",
        "<br/><br/>\n",
        "<br/><br/>\n",
        "\n",
        "<h1 align=\"right\">\n",
        "Artur Kalinowski <br>\n",
        "Uniwersytet Warszawski <br>\n",
        "Wydział Fizyki <br>    \n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6Lk6czWwdIAx"
      },
      "source": [
        "Istnieje, niekompletny, zbiór standardowych operacji jakie wykonujemy na różnego typu danych zanim zostaną użyte jako wejście do modelu.\n",
        "API Keras dostarcza gotowych warstw wykonujących wiele z tych [operacji](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
        "W tym notatniku użyjemy kilku z nich dla różnych rodzajów danych: **numerycznych**, **tekstowych**, **obrazów**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Yp8raTezdIAx"
      },
      "source": [
        "## Dane numeryczne\n",
        "\n",
        "### Normalizacja\n",
        "\n",
        "Standardowa operacja, jaką wykonujemy na danych numerycznych przez podaniem ich na wejście modelu to normalizacja.\n",
        "Normalizacja powoduje że rząd wielkości wag jest podobny dla wszystkich cech, a same wagi nei są zbyt duże.\n",
        "\n",
        "```Python\n",
        "\n",
        "normalization = tf.keras.layers.Normalization(mean, variance) # Normalizacja danych do średniej mean i wariancji wariance\n",
        "                                                               # domyślnie mean=0, variance=1\n",
        "                                                               # normalizacja przebiega dla każdej cechy oddzielnie\n",
        "                                                               # wymaga ustalenia współczynników normalizacji przez metodę adapt(x)\n",
        "normalization.adapt(x)                                                             \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "CZnrNaIVdIAx"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* wygenerować zbiór `(n,4)` liczb pochodzących z rozkładu płaskiego w zakresach `[[-5,5],[-4,2],[2,2]]`\n",
        "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w zbiorze\n",
        "* znormalizować dane do zakresu `[0,1]` dla każdej cechy oddzielnie\n",
        "* wypisać na ekran wartości minimalne, maksymalne  i średnią cech w znormalizowanym zbiorze\n",
        "* sprawdzić czy normalizacja zadziałała zgodnie z oczekiwaniem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "N6t23EbUdIAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae30fc89-a141-46a4-fce6-818adbdd70be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimalne wartości cech: [-4.99349228 -3.99711238  2.        ]\n",
            "Maksymalne wartości cech: [4.9894425  1.98788596 2.        ]\n",
            "Średnie wartości cech: [ 0.14442858 -0.96042834  2.        ]\n",
            "\n",
            "Minimalne wartości cech po normalizacji: [ 0.  0. nan]\n",
            "Maksymalne wartości cech po normalizacji: [ 1.  1. nan]\n",
            "Średnie wartości cech po normalizacji: [0.51467038 0.5073826         nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-779b306e53fe>:15: RuntimeWarning: invalid value encountered in divide\n",
            "  normalized_data[:, i] = (data[:, i] - min_val) / (max_val - min_val)\n"
          ]
        }
      ],
      "source": [
        "n = 10_00\n",
        "\n",
        "data = np.random.uniform(low=[-5, -4, 2], high=[5, 2, 2], size=(n, 3))\n",
        "\n",
        "# Wypisanie wartości minimalnych, maksymalnych i średniej\n",
        "print(colored(\"Minimalne wartości cech:\", \"blue\"), np.min(data, axis=0))\n",
        "print(colored(\"Maksymalne wartości cech:\", \"blue\"), np.max(data, axis=0))\n",
        "print(colored(\"Średnie wartości cech:\", \"blue\"), np.mean(data, axis=0))\n",
        "\n",
        "# Normalizacja danych\n",
        "normalized_data = data.copy()\n",
        "for i in range(data.shape[1]):\n",
        "      min_val = np.min(data[:, i])\n",
        "      max_val = np.max(data[:, i])\n",
        "      normalized_data[:, i] = (data[:, i] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Wypisanie wartości minimalnych, maksymalnych i średniej po normalizacji\n",
        "print(colored(\"\\nMinimalne wartości cech po normalizacji:\", \"blue\"), np.min(normalized_data, axis=0))\n",
        "print(colored(\"Maksymalne wartości cech po normalizacji:\", \"blue\"), np.max(normalized_data, axis=0))\n",
        "print(colored(\"Średnie wartości cech po normalizacji:\", \"blue\"), np.mean(normalized_data, axis=0))\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"rozwiazanie Kalinowskiego, inny rodzaj normalizacji do wariancji 1, srednia zero\"\n",
        "x = tf.random.uniform([n, 3])\n",
        "scales = np.array([[-5, 5], [-4, 2], [-2, 2]])\n",
        "ranges = scales[:, 1] - scales[:, 0]\n",
        "x = x * ranges + scales[:, 0]\n",
        "print(\"min\", tf.math.reduce_min(x, axis=0))\n",
        "print(\"max\", tf.math.reduce_max(x, axis=0))\n",
        "print(\"mean\", tf.math.reduce_mean(x, axis=0))\n",
        "\n",
        "normalization = tf.keras.layers.Normalization()\n",
        "normalization.adapt(x)\n",
        "x = normalization(x)\n",
        "print(\"min\", tf.math.reduce_min(x, axis=0))\n",
        "print(\"max\", tf.math.reduce_max(x, axis=0))\n",
        "print(\"mean\", tf.math.reduce_mean(x, axis=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OQQ_3yepsW0",
        "outputId": "438f014d-0e6e-4803-e302-96d82a4f8fb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min tf.Tensor([-4.9964275 -3.9962928 -1.9983568], shape=(3,), dtype=float32)\n",
            "max tf.Tensor([4.9984913 1.9982934 1.9943242], shape=(3,), dtype=float32)\n",
            "mean tf.Tensor([-0.02906598 -0.9558474   0.07405888], shape=(3,), dtype=float32)\n",
            "min tf.Tensor([-1.7083684 -1.7710953 -1.81101  ], shape=(3,), dtype=float32)\n",
            "max tf.Tensor([1.7290709 1.7208219 1.6780514], shape=(3,), dtype=float32)\n",
            "mean tf.Tensor([ 2.5987624e-08  1.6099214e-07 -1.1861324e-08], shape=(3,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ic2q-sT3dIAy"
      },
      "source": [
        "\n",
        "### Dyskretyzacja\n",
        "\n",
        "Czasami użyteczny jest podział danych numerycznych na kategorie - **dyskretyzacja**.\n",
        "W sytuacji, gdy nie potrzebujemy dużej rozdzielczości wartości zmiennoprzecinkowe możemy podzielić\n",
        "np. na `małe`, `średnie` i `duże`.\n",
        "Redukcja rozdzielczości z poziomu zmiennoprzecinkowego do listy kategorii może też ułatwić trening.\n",
        "\n",
        "```Python\n",
        "\n",
        "discretization = tf.keras.layers.Discretization(num_bins, bin_boundaries, output_mode)\n",
        "                 # Zamiana zmiennej ciągłej na dyskretną w postaci:\n",
        "                 # output_mode = int - numer przedziału (wartość domyślna)\n",
        "                 #               one_hot - wektor typu kodowania gorącojedynkowego\n",
        "                 # num_bins - liczba przedziałów (wymaga zawołania metody adapt(x))\n",
        "                 # bin_boundaries - zakresy przedziałów\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3_yoaNcbdIAy"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* zdyskretyzować dane z poprzedniej komórki do 10 przedziałów\n",
        "* narysować histogram numerów przedziałów dla **wszystkich** cech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "IScyAVcfdIAy"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ahI6GXTbdIAz"
      },
      "source": [
        "## Obrazy\n",
        "\n",
        "**Proszę:**\n",
        "\n",
        "* korzystając z biblioteki `tensorflow_datasets` załadować zbiór `imagenette/160px`\n",
        "* narysować kilka przykładowych rysunków"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "keJgyIU-dIAz"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "x_LzhGifdIAz"
      },
      "source": [
        "\n",
        "### Skalowanie\n",
        "\n",
        "Zmiana rozdzielczości - skalowanie obrazu. Skalowanie wymaga podania algorytmu interpolacji, pozwalającego\n",
        "na obliczenie wartości pikseli w nowym obrazie.\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.Resizing(\n",
        "    height, width,                # szerokość i wysokość nowego obrazu\n",
        "    interpolation='bilinear',     # algorytm interpolacji\n",
        "    crop_to_aspect_ratio=False,   # przycinanie obrazu w celu uzyskania\n",
        "                                  # tego samego stosunku szerokość/długość\n",
        "                                  # jak w obrazie oryginalnym\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "QPk1VNHkdIAz"
      },
      "source": [
        "## Przycinane\n",
        "\n",
        "z całego obrazu jest wycinany fragment, `ramka`:\n",
        "\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.CenterCrop(\n",
        "    height, width              # szerokość i wysokość prostokąta wycinającego\n",
        "                               # fragment w środku obrazu\n",
        ")\n",
        "```\n",
        "\n",
        "Przycinanie w losowym miejscu może być użyte do wzbogacania próbki, poprzez generację\n",
        "losowych fragmentów obrazu - ang. `augmenting`. Warstwy wykonujące losowe operacje na obrazach\n",
        "są domyślnie aktywne tylko w czasie treningu.\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.RandomCrop(\n",
        "    height, width, seed=None,  # szerokość i wysokość prostokąta wycinającego\n",
        "                               # fragment w losowym miejscu\n",
        "                               #\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "sAVmr5e2dIAz"
      },
      "source": [
        "\n",
        "\n",
        "### Obrót\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.RandomRotation(\n",
        "    factor,                         # zakres obrotu w jednostkach 2pi: (min, max)\n",
        "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po obrocie obrazu\n",
        "    interpolation='bilinear',\n",
        "    seed=None,\n",
        "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
        "                                    # jeśli jako `fill_mode=constant`\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Vcr14v7qdIAz"
      },
      "source": [
        "\n",
        "### Translacja\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.RandomTranslation(\n",
        "    height_factor,                  # względny współczynnik przesunięcia w pionie: (min, max)\n",
        "    width_factor,                   # względny współczynnik przesunięcia w poziomie: (min, max)\n",
        "    fill_mode='reflect',            # algorytm wypełnienia przestrzeni powstałej po przesunięciu obrazu\n",
        "    interpolation='bilinear',\n",
        "    seed=None,\n",
        "    fill_value=0.0,                 # wartość piksela użytego do wypełniania przestrzeni powstałej po przesunięciu obrazu,\n",
        "                                    # jeśli jako `fill_mode=constant`\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "E96huzk1dIAz"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
        "* skalowaniu obszaru do rozdzielczości `(320,320)`\n",
        "* wypisać na ekran rozdzielczość pierwszego przykładu\n",
        "\n",
        "**Wskazówki:**\n",
        "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o odpowiednią warstwę\n",
        "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "KoYNoamtdIAz"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "fjW82276dIA0"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
        "* przycinaniu do obszaru centralnego o rozmiarze `(64,64)`\n",
        "\n",
        "**Wskazówki:**\n",
        "* należy użyć metody `tf.data.Dataset.map()` z odpowiednią funkcją mapowania opartą o `tf.keras.layers.CenterCrop`\n",
        "* uwaga na typ danych w tensorze zawierającym przetworzone obrazy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Jn6mn79xdIA0"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "rIV_LHdidIA0"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
        "\n",
        "* losowemu przycinaniu do obszaru o rozmiarze `(64,64)`\n",
        "\n",
        "**Wskazówki:**\n",
        "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "SK5R1KpvdIA0"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ZARM-EvkdIA0"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "Narysować losowe obrazy ze zbioru `imagenette/160px` poddane:\n",
        "\n",
        "* losowemu obrotowi w zakresie $\\pm \\pi/4$\n",
        "* puste miejsca po obrocie proszę wypełnić kolorem czarnym\n",
        "\n",
        "**Wskazówki:**\n",
        "* użycie warstwy w definicji funkcji lambda spowoduje błędy. Proszę spróbować zinterpretować komunikat o błędzie i odpowiednio skorygować kod.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "_QxPv4bBdIA0"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "dn5oQiMtdIA0"
      },
      "source": [
        "## Dane tekstowe\n",
        "\n",
        "Zamiana tekstu na postać cyfrową może być wykonana na wiele sposobów. Dwa najbardziej popularne to:\n",
        "* **wektoryzacja (ang. text vectorization)** - każdemu znacznikowi (ang. `token`) jest przypisana liczba całkowita, indeks w słowniku.\n",
        "                 Odwzorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest ustalane na podstawie zawartości zbioru danych.\n",
        "\n",
        "* **zanurzanie (ang. embedding)** - każdemu znacznikowi jest przypisany n-wymiarowy wektor liczb zmiennoprzecinkowych.\n",
        "    Odwzorowanie   ${\\mathrm tekst}  \\leftrightarrow {\\mathrm indeks}$ jest znajdowanie w czasie treningu modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "wLT42CekdIA0"
      },
      "source": [
        "### Wektoryzacja\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None,                           # maksymalna liczba znaczników w słowniku\n",
        "    standardize='lower_and_strip_punctuation', # algorytm standaryzacji tekstu\n",
        "    split='whitespace',                        # algorytm podziału na słowa\n",
        "    ngrams=None,                               # algorytm podziału słów na n-literowe fragmenty\n",
        "    output_mode='int',                         # typ wyjścia   \n",
        "    output_sequence_length=None,               # maksymalna długość zakodowanej sekwencji \"zdania\"\n",
        "    pad_to_max_tokens=False,                   # czy dopełniać sekwencję zerami do maksymalnej długości\n",
        "    vocabulary=None                            # słownik. Jeśli nie jest podany generacja słownika wymaga zawołania\n",
        "                                               # metody adapt()\n",
        ")\n",
        " ```\n",
        "\n",
        " Znaczniki nie występujące w słowniku otrzymają ten sam indeks oznaczający znacznik OOV (`ang. out of vocabulary`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "hpjBwFqJdIA0"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* zbudować słownik na tekście `wksf/Korpus_surowy` wczytywanym w ramach zadania domowego z poprzedniego notatnika\n",
        "* zwektoryzować teskt `Król zasiada na tronie.`\n",
        "* wypisać na ekran zwektoryzowaną postać\n",
        "* przeprowadzić operację odwrotną - z postaci zwektoryzowanej odtworzyć tekst\n",
        "* powtórzyć procedurę dla tekstu `Ania ma małego kotka.`\n",
        "\n",
        "**Wskazówki**:\n",
        "* słownik utworzony przez warstwę `tf.keras.layers.TextVectorization` uzyskujemy przez metodę `get_vocabulary()`\n",
        "* z elementów sekwencji `words` można utworzyć napis w następujący sposób:\n",
        "```Python\n",
        "sentence = \" \".join(words)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6iwsWhQIdIA1"
      },
      "outputs": [],
      "source": [
        "import text_functions as txt_fcn\n",
        "\n",
        "filePath = \"../data/wksf/Korpus_surowy/\"\n",
        "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
        "\n",
        "...rozwiązanie...\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "KnfU3l12dIA1"
      },
      "source": [
        "### Zanurzanie\n",
        "\n",
        "\n",
        "```Python\n",
        "tf.keras.layers.Embedding(\n",
        "    input_dim,                          # rozmiar słownika - liczba znaczników (\"tokenów\")\n",
        "    output_dim,                         # wymiar reprezentacji  \n",
        ")\n",
        "```\n",
        "\n",
        "Warstwa zanurzająca przypisuje wartość zmiennoprzecinkową każdemu znacznikowi.\n",
        "Taką operację można reprezentować przez macierz `(output_dim, input_dim)` która działa na wektor gorącojedynkowy o długości `(input_dim)`\n",
        "i produkuje reprezentację zmiennoprzecinkową o długości `output_dim`\n",
        "Tutaj `output_dim=3`:\n",
        "\n",
        "$$\n",
        "\\huge{\n",
        "\\begin{bmatrix}\n",
        "a_{0} & b_{0} & c_{0} & \\dots \\\\\n",
        "a_{1} & b_{1} & c_{1} & \\dots \\\\\n",
        "a_{2} & b_{2} & c_{2} & \\dots \\\\\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "\\dots \\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "a_{0} \\\\\n",
        "a_{1} \\\\\n",
        "a_{2}\n",
        "\\end{bmatrix}\n",
        "}\n",
        "$$\n",
        "Warstwa `tf.keras.layers.Enbedding()` realizuje tę operację w sposób zoptymalizowany.\n",
        "Macierz zanurzania jest zwykle zmieniana w trakcie treningu modelu który ją zawiera, więc nie jest to standardowa warstwa wstępnego przetwarzania.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ErXM_v05dIA1"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* zwektoryzować teskt `Król zasiada na tronie.`\n",
        "* zwektoryzowaną postać podać na wejście warstwy zanurzającej z `nDims = 4`\n",
        "* wypisać na ekran obie postacie tekstu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "yQ7hmLEzdIA1"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4gnDDuRmdIA1"
      },
      "source": [
        "### Podział na n-gramy\n",
        "\n",
        "W czasie analizy tekst zwykle dzieli się na fragmenty zawierające `n` znaczników (ang. `tokens`) - n-gramy.\n",
        "Podzielimy zdania wczytane z korpusu języka polskiego na odcinki o długości `n` wyrazów. Skorzystamy z gotowych funkcji służących\n",
        "do operowania na napisach dostępnych w dedykowanej bibliotece `tensorflow_text`\n",
        "\n",
        "* podział tekstu na fragmenty (tutaj wyrazy oddzielone spacją):\n",
        "```Python\n",
        "tensorflow_text.WhitespaceTokenizer().tokenize(text)\n",
        "```\n",
        "\n",
        "* tworzenie grup o wybranej długości z użyciem biegnącego okna - grupy się przekrywają za wyjątkiem ostatniego wyrazu, czyli krok okna ang. `stride`\n",
        "wynosi 1\n",
        "```Python\n",
        "tensorflow_text.tf_text.sliding_window(data,       # lista znaczników        \n",
        "                                       width,      # szerokość okna przebiegającego listę\n",
        "                                       axis=-1,    # wymiar, wzdłuż którego biegnie okno\n",
        "                                       name=None   # nazwa funkcji\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "J6ytArlEdIA1"
      },
      "outputs": [],
      "source": [
        "import tensorflow_text as tf_text\n",
        "import functools\n",
        "\n",
        "# load the dataset\n",
        "filePath = \"../data/wksf/Korpus_surowy/\"\n",
        "dataset = txt_fcn.load_wksf_dataset(filePath)\n",
        "\n",
        "# split lines into words\n",
        "dataset = dataset.map(tf_text.WhitespaceTokenizer().tokenize)\n",
        "\n",
        "# remove empty lines\n",
        "dataset = dataset.filter(lambda x: tf.size(x) > 0)\n",
        "\n",
        "# fix all function arguments except for the input data\n",
        "window_size = 5\n",
        "slidingWindowWithWidth = functools.partial(tf_text.sliding_window, width=window_size)\n",
        "\n",
        "# apply the sliding window to each line.\n",
        "# this will priduce a tensor of shape (n, width) for each line,\n",
        "# where n in the number of groups of words of words of width length\n",
        "dataset = dataset.map(slidingWindowWithWidth)\n",
        "\n",
        "print(colored(\"First example:\", \"blue\"))\n",
        "for item in dataset.take(1):\n",
        "    print(colored(\"Text: \", \"blue\"), item.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "bGdxudPLdIA1"
      },
      "source": [
        "Po podziale na n-gramy z jednego wiersza zrobiło się wiele fragmentów o długości `window_size`. Możemy je traktować jako paczki.\n",
        "Operacja rozpaczkowania, `dataset.unbatch()` z powrotem przywróci zbiór do `postaci jeden przykład na wiersz`.\n",
        "\n",
        "Rozpaczkowane wyrazy możemy z powrotem połączyć w fragmenty zdań.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "G9MFixzPdIA1"
      },
      "outputs": [],
      "source": [
        "# split the (n, width) tensor into (n) tensors of shape (width)\n",
        "dataset = dataset.unbatch()\n",
        "\n",
        "# merge words into sentence framgents\n",
        "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, separator=' '))\n",
        "\n",
        "print(colored(\"First five five-word blocks:\", \"blue\"))\n",
        "for item in dataset.take(5):\n",
        "    print(colored(\"Text: \", \"blue\"), item.numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "GVnqVGYjdIA2"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* stworzyć słownik do wektoryzacji tekstu używając warstwy `tf.keras.layers.TextVectorization.adapt(...)`\n",
        "* stworzyć zwektoryzowany zbiór danych: `dataset_vectorized` używając warstwy `tf.keras.layers.TextVectorization` i operacji `dataset.map()`\n",
        "* zachować słownik w zmiennej `vocabulary` w postaci macierzy numpy\n",
        "* usunąć przykłady z miej niż dwo wyrazami w zdaniu\n",
        "* wypisać na ekran liczbę znaczników w słowniku\n",
        "* wypisać na ekran pięć pierwszych przykładów w zwektoryzowanej postaci\n",
        "\n",
        "**Wskazówka:**\n",
        "* operacje na zbiorach danych można przyspieszyć wykonując je na paczkach:\n",
        "```Python\n",
        "dataset.batch(n).map(...).unbatch()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Ja_4eYU3dIA2"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "\n",
        "print(colored(\"First five five-word blocks in the vectorized form:\", \"blue\"))\n",
        "for item in dataset_vectorized.take(5):\n",
        "    print(colored(\"Text: \", \"blue\"), item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "NKIqoOlGdIA2"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* przekształcić zwektoryzowany zbiór zawierający n-gramy do postaci `(cechy, etykieta)` gdzie:\n",
        "    * **etykieta** - środkowy wyraz\n",
        "    * **cechy** - wyrazy poza wyrazem środkowym\n",
        "\n",
        "* przekształcenie powinno korzystać z metody `Dataset.map(...)` z użyciem własnej funkcji mapującej `map_fn(...)`\n",
        "* założyć, że zbiór został podzielony na paczki, więc pojedynczy element ma kształt `(None,width)`\n",
        "* wypisać na ekran cechy i etykiety dla pięciu przykładów\n",
        "\n",
        "**Wskazówki**:\n",
        "* można założyć że `n=5`\n",
        "* można założyć, że środkowy wyraz ma indeks `2`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Qqmx0VRvdIA2"
      },
      "outputs": [],
      "source": [
        "###################################################\n",
        "def map_fn(x):\n",
        "...rozwiązanie...\n",
        "    return features, label\n",
        "###################################################\n",
        "def print_item(batch, vocabulary, width=2):\n",
        "    batch_index = 0\n",
        "    item = (batch[0][batch_index], batch[1][batch_index])\n",
        "    features = \" \".join(vocabulary[item[0].numpy()[0:width]])\n",
        "    label = vocabulary[item[1].numpy()]\n",
        "    print(colored(\"Features\", \"blue\"), end=\" \")\n",
        "    print(colored(\"(Label):\", \"red\"), end=\" \")\n",
        "\n",
        "    print(features, end=\" \")\n",
        "    print(colored(label,\"red\"), end=\" \")\n",
        "    features = \" \".join(vocabulary[item[0].numpy()[width:]])\n",
        "    print(features)\n",
        "###################################################\n",
        "\n",
        "dataset_final = dataset_vectorized.batch(32).map(map_fn)\n",
        "\n",
        "for item in dataset_final.take(5):\n",
        "    print_item(item, vocabulary)\n",
        "    print(colored(\"Vectorized form:\", \"blue\"), )\n",
        "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
        "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9IYtSUtBdIA5"
      },
      "source": [
        "**Proszę:**\n",
        "\n",
        "* sprawdzić prędkość czytania finalnego zbioru danych korzystając z funkcji `benchmark`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "jSecbrCHdIA5"
      },
      "outputs": [],
      "source": [
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Z5wDQql-dIA5"
      },
      "source": [
        "# Zadanie domowe\n",
        "\n",
        "**Proszę:**\n",
        "\n",
        "* załadować tekst z pliku filePath = `shakespeare.txt'`\n",
        "\n",
        "```Python\n",
        "\n",
        "filePath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "```\n",
        "\n",
        "* wykonać `preprocessing` tekstu:\n",
        "    * podział tekstu na fragmenty o długości pięciu wyrazów. Jeden przykład w nowym zbiorze powinien składać się jednego 5-wyrazowego fragmentu,\n",
        "      a nie grupy fragmentów powstałej z podziału zdania na kawałki o długości pięciu wyrazów:\n",
        "      ```\n",
        "      \n",
        "      Features (Label): before we proceed any further\n",
        "      Features:  [128  33 123 639] Label:  1267\n",
        "      ```\n",
        "    * tokenizacja ze słownikiem ograniczonym do **1000** znaczników\n",
        "    * podział fragmentów na etykietę (wyraz środkowy) i cechy (pozostałe wyrazy)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "eOAIv2B7dIA5"
      },
      "source": [
        "* wypisać na ekran pięć przykładów z zaznaczeniem cech i etykiety\n",
        "* stworzyć warstwę zanurzającą ze `128` wymiarami\n",
        "* wypisać na ekran pięć wyrazów najbliższych wyrazowi `man` w przestrzeni zanurzającej z odległością kosinusową:\n",
        "```Python\n",
        "cosine_similarity = tf.keras.losses.cosine_similarity(...)\n",
        "```\n",
        "* wypisać na ekran pięć słów najbliższych do sumy słów `mother` i `father` wykonanej w przestrzeni zanurzenia\n",
        "\n",
        "**Wskazówki:**\n",
        "* największe `n` wartości z listy można uzyskać funkcją `tf.math.top_k(...)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8EfZytycdIA5"
      },
      "source": [
        "<hr>\n",
        "\n",
        "**Opcjonalnie:**\n",
        "\n",
        "* przeprowadzić trening warstwy zanurzającej ze `128` wymiarami z użyciem algorytmu ciągłego worka słów - [`Continous Bag of Words (CBOW)`](https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#the-continuous-bag-of-words-cbow-model) (wersja naiwna).\n",
        "\n",
        "**Wskazówki:**\n",
        "\n",
        "* obliczenie iloczynu skalarnego reprezentacji cech i wszystkich słów słownika wymaga zdefiniowania warstwy liczącej iloczyn skalarny:\n",
        "```Python\n",
        "class Dot(tf.keras.Layer):\n",
        "    def call(self, x):\n",
        "        dot_product = ...\n",
        "        return dot_product\n",
        "\n",
        "```\n",
        "i użycia jej w definicji modelu.\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "u_cpAROFdIA5"
      },
      "outputs": [],
      "source": [
        "# load text\n",
        "...rozwiązanie...\n",
        "\n",
        "# adapt vextorization layer\n",
        "...rozwiązanie...\n",
        "\n",
        "# split lines into words\n",
        "...rozwiązanie...\n",
        "\n",
        "# fix all tf_text.sliding_window function arguments except for the input data\n",
        "...rozwiązanie...\n",
        "\n",
        "# apply the sliding window to each line.\n",
        "# this will produce a tensor of shape (n, width) for each line,\n",
        "# where n in the number of groups of words with length width\n",
        "...rozwiązanie...\n",
        "\n",
        "# remove empty lines\n",
        "...rozwiązanie...\n",
        "\n",
        "# split the (n, width) tensor into (n) tensors of shape (width)\n",
        "...rozwiązanie...\n",
        "\n",
        "# merge words into sentence framgents\n",
        "...rozwiązanie...\n",
        "\n",
        "#Vectorize\n",
        "...rozwiązanie...\n",
        "\n",
        "for item in dataset_final.take(5):\n",
        "    print_item(item, vocabulary, width=2)\n",
        "    print(colored(\"Features: \", \"blue\"), item[0][0].numpy(), end=\" \")\n",
        "    print(colored(\"Label: \", \"blue\"), item[1][0].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "A0iwC9SwdIA6"
      },
      "outputs": [],
      "source": [
        "%time\n",
        "# CBOW model training (optional)\n",
        "...rozwiązanie...\n",
        "\n",
        "#Evaluate non trained model\n",
        "model.evaluate(dataset_final.take(16))\n",
        "\n",
        "#Training\n",
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ltdIjig1dIA6"
      },
      "outputs": [],
      "source": [
        "# Embeding space exploration - words similar to \"man\"\n",
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5E9jhwi5dIA6"
      },
      "outputs": [],
      "source": [
        "# Word arithmetics - words similar to \"mother\" + \"father\"\n",
        "words = np.array([\"father\", \"mother\"])\n",
        "...rozwiązanie...\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHaxRio4dIA6"
      },
      "source": [
        "Odwzorowanie zanurzenia można zwizualizować z użyciem portalu [Embeding Projector](http://projector.tensorflow.org/?_gl=1*u2l7wh*_ga*MTg4NTM3NDUwOC4xNzA3OTg4NTU4*_ga_W0YLR4190T*MTcxNTI0MzQxOC44Ny4xLjE3MTUyNDQ5NzMuMC4wLjA.)\n",
        "Na stronę trzeba załadować pliki `vectors.tsv` i `metadata.tsv` uzyskane z warstwy zanurzającej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "daUvMpLudIA6"
      },
      "outputs": [],
      "source": [
        "def dump_embedding(model, vocabulary):\n",
        "  import io\n",
        "  out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "  out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "  weights = model.get_layer('embedding').get_weights()[0]\n",
        "  for index, word in enumerate(vocabulary):\n",
        "    if index == 0:\n",
        "      continue  # skip 0, it's padding.\n",
        "    vec = weights[index]\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "    out_m.write(word + \"\\n\")\n",
        "  out_v.close()\n",
        "  out_m.close()\n",
        "\n",
        "dump_embedding(model, vocabulary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "rise": {
      "center": false,
      "controls": false,
      "footer": "<h3>Letnia Szkoła<br>Fizyki 2023</h3>",
      "header": "<h1>Hello</h1>",
      "progress": "true",
      "slideNumber": "c/t",
      "transition": "none"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}